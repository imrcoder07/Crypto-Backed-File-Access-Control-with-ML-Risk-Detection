{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8229c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ADVANCED Full Project Pipeline ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"--- Starting ADVANCED Full Project Pipeline ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a014cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directories verified/created.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuration & Setup ---\n",
    "# =================================\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "VIS_DIR = 'visualizations'\n",
    "\n",
    "FILE_CSV = os.path.join(DATA_DIR, 'file.csv')\n",
    "USERS_CSV = os.path.join(DATA_DIR, 'users.csv')\n",
    "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, 'processed_data.csv')\n",
    "\n",
    "RF_PIPELINE_PATH = os.path.join(MODELS_DIR, 'random_forest_pipeline.pkl')\n",
    "SVM_PIPELINE_PATH = os.path.join(MODELS_DIR, 'svm_pipeline.pkl')\n",
    "ISO_PIPELINE_PATH = os.path.join(MODELS_DIR, 'isolation_forest_pipeline.pkl')\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(VIS_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ Directories verified/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d370c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 1: Starting Data Analysis & Visualization ---\n",
      "Analyzing users.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk521\\AppData\\Local\\Temp\\ipykernel_19648\\808445227.py:9: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=top_roles.values, y=top_roles.index, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> ‚úÖ Chart '1_top_job_roles.png' saved.\n",
      "Analyzing file.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk521\\AppData\\Local\\Temp\\ipykernel_19648\\808445227.py:30: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=top_file_activities.values, y=top_file_activities.index, palette='magma')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> ‚úÖ Chart '2_top_file_activities.png' saved.\n",
      "  -> ‚úÖ Chart '3_activity_by_hour.png' saved.\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Exploratory Data Analysis & Visualization ---\n",
    "# ======================================================\n",
    "print(\"\\n--- Phase 1: Starting Data Analysis & Visualization ---\")\n",
    "try:\n",
    "    print(\"Analyzing users.csv...\")\n",
    "    users_df_analysis = pd.read_csv(USERS_CSV)\n",
    "    top_roles = users_df_analysis['role'].value_counts().nlargest(10)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=top_roles.values, y=top_roles.index, palette='viridis')\n",
    "    plt.title('Top 10 Most Common Job Roles', fontsize=16)\n",
    "    plt.xlabel('Number of Employees', fontsize=12)\n",
    "    plt.ylabel('Job Role', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIS_DIR, '1_top_job_roles.png'))\n",
    "    plt.close()\n",
    "    print(\"  -> ‚úÖ Chart '1_top_job_roles.png' saved.\")\n",
    "\n",
    "    print(\"Analyzing file.csv...\")\n",
    "    chunk_reader_analysis = pd.read_csv(FILE_CSV, usecols=['activity', 'date'], chunksize=500_000)\n",
    "    all_activities = pd.Series(dtype='int64')\n",
    "    hourly_activity = pd.Series(dtype='int64', index=range(24)).fillna(0)\n",
    "    for chunk in chunk_reader_analysis:\n",
    "        all_activities = all_activities.add(chunk['activity'].value_counts(), fill_value=0)\n",
    "        chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')\n",
    "        hourly_counts = chunk['date'].dt.hour.value_counts()\n",
    "        hourly_activity = hourly_activity.add(hourly_counts, fill_value=0)\n",
    "\n",
    "    top_file_activities = all_activities.nlargest(10).astype(int)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=top_file_activities.values, y=top_file_activities.index, palette='magma')\n",
    "    plt.title('Top 10 Most Common File Activities', fontsize=16)\n",
    "    plt.xlabel('Total Count', fontsize=12)\n",
    "    plt.ylabel('Activity Type', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIS_DIR, '2_top_file_activities.png'))\n",
    "    plt.close()\n",
    "    print(\"  -> ‚úÖ Chart '2_top_file_activities.png' saved.\")\n",
    "\n",
    "    hourly_activity = hourly_activity.fillna(0).astype(int)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=hourly_activity.index, y=hourly_activity.values, color='royalblue')\n",
    "    plt.title('File System Activity by Hour of the Day', fontsize=16)\n",
    "    plt.xlabel('Hour of Day (0-23)', fontsize=12)\n",
    "    plt.ylabel('Number of Activities', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIS_DIR, '3_activity_by_hour.png'))\n",
    "    plt.close()\n",
    "    print(\"  -> ‚úÖ Chart '3_activity_by_hour.png' saved.\")\n",
    "    print(\"--- Analysis Complete ---\")\n",
    "except Exception as e:\n",
    "    print(f\"  -> ‚ùå Error during analysis phase: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5496d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Starting Advanced Data Processing ---\n",
      "  -> Step 2a: Calculating user activity baselines...\n",
      "  -> ‚úÖ User baselines calculated.\n",
      "  -> Step 2b: Processing data chunks with new features...\n",
      "    -> Processed chunk 5...\n",
      "\n",
      "Loading final processed dataset...\n",
      "‚úÖ Final dataset with advanced features loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. ADVANCED Data Processing & Feature Engineering ---\n",
    "# =======================================================\n",
    "print(\"\\n--- Phase 2: Starting Advanced Data Processing ---\")\n",
    "try:\n",
    "    users_df = pd.read_csv(USERS_CSV, usecols=['user_id', 'role'])\n",
    "    if os.path.exists(PROCESSED_DATA_PATH): os.remove(PROCESSED_DATA_PATH)\n",
    "    \n",
    "    print(\"  -> Step 2a: Calculating user activity baselines...\")\n",
    "    chunk_reader_baseline = pd.read_csv(FILE_CSV, usecols=['user', 'date'], chunksize=500_000)\n",
    "    user_activity_counts = pd.Series(dtype='int64')\n",
    "    for chunk in chunk_reader_baseline:\n",
    "        user_activity_counts = user_activity_counts.add(chunk['user'].value_counts(), fill_value=0)\n",
    "    \n",
    "    user_baselines = pd.DataFrame({'user': user_activity_counts.index, 'total_actions': user_activity_counts.values})\n",
    "    user_baselines['avg_actions_per_day'] = user_baselines['total_actions'] / user_baselines['total_actions'].max()\n",
    "    print(\"  -> ‚úÖ User baselines calculated.\")\n",
    "\n",
    "    print(\"  -> Step 2b: Processing data chunks with new features...\")\n",
    "    chunk_reader_process = pd.read_csv(FILE_CSV, usecols=['user', 'activity', 'date'], chunksize=500_000)\n",
    "    features_to_keep = ['activity', 'role', 'hour_of_day', 'day_of_week', 'is_weekend', 'avg_actions_per_day', 'risky']\n",
    "\n",
    "    for i, chunk in enumerate(chunk_reader_process):\n",
    "        merged_chunk = pd.merge(chunk, users_df, left_on='user', right_on='user_id', how='left')\n",
    "        merged_chunk = pd.merge(merged_chunk, user_baselines[['user', 'avg_actions_per_day']], on='user', how='left')\n",
    "        merged_chunk['date'] = pd.to_datetime(merged_chunk['date'], errors='coerce')\n",
    "        merged_chunk['hour_of_day'] = merged_chunk['date'].dt.hour\n",
    "        merged_chunk['day_of_week'] = merged_chunk['date'].dt.dayofweek\n",
    "        merged_chunk['is_weekend'] = merged_chunk['day_of_week'].isin([5, 6]).astype(int)\n",
    "        merged_chunk['risky'] = ((merged_chunk['hour_of_day'] < 8) | (merged_chunk['hour_of_day'] > 18) | (merged_chunk['is_weekend'] == 1)).astype(int)\n",
    "        \n",
    "        processed_chunk = merged_chunk[features_to_keep].dropna()\n",
    "        processed_chunk.to_csv(PROCESSED_DATA_PATH, mode='a', index=False, header=(i == 0))\n",
    "        if (i+1) % 5 == 0: print(f\"    -> Processed chunk {i+1}...\")\n",
    "\n",
    "    print(\"\\nLoading final processed dataset...\")\n",
    "    df_model = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "    print(\"‚úÖ Final dataset with advanced features loaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during data processing: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e032bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3: Preparing Data and Building Preprocessing Pipeline ---\n",
      "‚öôÔ∏è Dataset sampled for faster training.\n",
      "‚úÖ Preprocessing pipeline created.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Advanced Preprocessing with Pipelines ---\n",
    "# ================================================\n",
    "print(\"\\n--- Phase 3: Preparing Data and Building Preprocessing Pipeline ---\")\n",
    "if len(df_model) > 200_000:\n",
    "    df_model = df_model.sample(n=200_000, random_state=42)\n",
    "    print(\"‚öôÔ∏è Dataset sampled for faster training.\")\n",
    "\n",
    "target = 'risky'\n",
    "X = df_model.drop(target, axis=1)\n",
    "y = df_model[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "categorical_features = ['activity', 'role']\n",
    "numerical_features = ['hour_of_day', 'day_of_week', 'is_weekend', 'avg_actions_per_day']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "print(\"‚úÖ Preprocessing pipeline created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9805ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4: Training All Models ---\n",
      "\n",
      "--- Training Random Forest Pipeline with Tuning ---\n",
      "Best RF Parameters: {'classifier__n_estimators': 150, 'classifier__max_depth': 30}\n",
      "Tuned Random Forest Accuracy: 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     36285\n",
      "           1       1.00      1.00      1.00      3715\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      1.00      1.00     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n",
      "‚úÖ Tuned Random Forest pipeline saved.\n",
      "‚è±  Execution Time: 102.67 seconds\n",
      "\n",
      "--- Training Linear SVM Pipeline ---\n",
      "Linear SVM Accuracy: 96.31%\n",
      "‚úÖ Linear SVM pipeline saved.\n",
      "‚è±  Execution Time: 1.49 seconds\n",
      "\n",
      "--- Training Isolation Forest Pipeline ---\n",
      "Isolation Forest Accuracy: 91.88%\n",
      "‚úÖ Isolation Forest pipeline saved.\n",
      "‚è±  Execution Time: 1.49 seconds\n",
      "\n",
      "\n",
      "üéØ All tasks complete. Your ADVANCED model pipelines are trained and saved! ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Train All 3 Models with ADVANCED Tuning ---\n",
    "# =====================================================\n",
    "print(\"\\n--- Phase 4: Training All Models ---\")\n",
    "\n",
    "# --- Model 1: Random Forest with Hyperparameter Tuning ---\n",
    "print(\"\\n--- Training Random Forest Pipeline with Tuning ---\")\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 150],\n",
    "    'classifier__max_depth': [10, 20, 30]\n",
    "}\n",
    "random_search = RandomizedSearchCV(rf_pipeline, param_distributions=param_dist, n_iter=4, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best RF Parameters: {random_search.best_params_}\")\n",
    "best_rf_pipeline = random_search.best_estimator_\n",
    "preds = best_rf_pipeline.predict(X_test)\n",
    "print(f\"Tuned Random Forest Accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")\n",
    "print(classification_report(y_test, preds))\n",
    "with open(RF_PIPELINE_PATH, 'wb') as f: pickle.dump(best_rf_pipeline, f)\n",
    "print(f\"‚úÖ Tuned Random Forest pipeline saved.\")\n",
    "print(f\"‚è±  Execution Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# --- Model 2: Linear SVM ---\n",
    "print(\"\\n--- Training Linear SVM Pipeline ---\")\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearSVC(max_iter=2000, class_weight='balanced', random_state=42, dual=False))\n",
    "])\n",
    "start_time = time.time()\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "preds = svm_pipeline.predict(X_test)\n",
    "print(f\"Linear SVM Accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")\n",
    "with open(SVM_PIPELINE_PATH, 'wb') as f: pickle.dump(svm_pipeline, f)\n",
    "print(f\"‚úÖ Linear SVM pipeline saved.\")\n",
    "print(f\"‚è±  Execution Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# --- Model 3: Isolation Forest ---\n",
    "print(\"\\n--- Training Isolation Forest Pipeline ---\")\n",
    "contamination = y_train.value_counts(normalize=True).get(1, 0.01)\n",
    "iso_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', IsolationForest(n_estimators=100, contamination=contamination, random_state=42, n_jobs=-1))\n",
    "])\n",
    "start_time = time.time()\n",
    "iso_pipeline.fit(X_train)\n",
    "raw_preds = iso_pipeline.predict(X_test)\n",
    "preds = [1 if p == -1 else 0 for p in raw_preds]\n",
    "print(f\"Isolation Forest Accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")\n",
    "with open(ISO_PIPELINE_PATH, 'wb') as f: pickle.dump(iso_pipeline, f)\n",
    "print(f\"‚úÖ Isolation Forest pipeline saved.\")\n",
    "print(f\"‚è±  Execution Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\\nüéØ All tasks complete. Your ADVANCED model pipelines are trained and saved! ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
